{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9861c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. Install dependencies (ensure correct versions)\n",
    "# =========================\n",
    "!pip install --upgrade PyMuPDF langchain langchain_experimental langchain-text-splitters sentence-transformers pinecone-client google-generativeai tqdm pillow\n",
    "\n",
    "# =========================\n",
    "# 2. Imports\n",
    "# =========================\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Import SemanticChunker from experimental\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# =========================\n",
    "# 3. Configuration\n",
    "# =========================\n",
    "PDF_FOLDER = \"/content/pdfs\"\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"AIzaSyC5XlpDGwiPOc-cHX1jKtFsMSVYd3xPGGM\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"pcsk_73fzGC_EkqGu38kPmvQGKPDv3pcZ4WQ4oDTAorq5uyEYsxsgWB9GaX34MMMCmGvFQiBWJp\")\n",
    "INDEX_BASE = \"my-multimodal-rag\"\n",
    "INDEX_TEXT = INDEX_BASE + \"-text\"\n",
    "INDEX_IMAGE = INDEX_BASE + \"-image\"\n",
    "\n",
    "PINECONE_REGION = \"us-east-1\"\n",
    "GEMINI_MODEL_NAME = \"gemini-pro-latest\"\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# =========================\n",
    "# 4. Load embedding models & chunker\n",
    "# =========================\n",
    "# text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "clip_model = SentenceTransformer(\"clip-ViT-B-32\")\n",
    "\n",
    "# Wrap your text model with HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Use semantic chunker with the wrapped embedding model\n",
    "semantic_chunker = SemanticChunker(embedding_model)\n",
    "\n",
    "# =========================\n",
    "# 5. PDF extraction of text + images\n",
    "# =========================\n",
    "def extract_from_pdf(pdf_path: Path):\n",
    "    texts = []\n",
    "    images = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pid = pdf_path.stem\n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        txt = page.get_text(\"text\")\n",
    "        if txt and txt.strip():\n",
    "            texts.append({\"paper_id\": pid, \"page\": page_num, \"text\": txt})\n",
    "        for img_idx, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            try:\n",
    "                base = doc.extract_image(xref)\n",
    "                img_bytes = base[\"image\"]\n",
    "                ext = base.get(\"ext\", \"png\")\n",
    "                pil = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "                images.append({\n",
    "                    \"paper_id\": pid,\n",
    "                    \"page\": page_num,\n",
    "                    \"image_index\": img_idx,\n",
    "                    \"pil_image\": pil,\n",
    "                    \"ext\": ext\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"Image extraction failed:\", pdf_path, page_num, img_idx, e)\n",
    "    doc.close()\n",
    "    return texts, images\n",
    "\n",
    "# =========================\n",
    "# 6. Load all PDFs\n",
    "# =========================\n",
    "pdfs = list(Path(PDF_FOLDER).glob(\"*.pdf\"))\n",
    "if not pdfs:\n",
    "    raise RuntimeError(f\"No PDF files in {PDF_FOLDER}\")\n",
    "\n",
    "all_texts = []\n",
    "all_images = []\n",
    "for p in tqdm(pdfs, desc=\"Extracting PDFs\"):\n",
    "    t, im = extract_from_pdf(p)\n",
    "    all_texts.extend(t)\n",
    "    all_images.extend(im)\n",
    "\n",
    "print(\"Text pages:\", len(all_texts), \"Images:\", len(all_images))\n",
    "\n",
    "# =========================\n",
    "# 7. Semantic chunking of texts\n",
    "# =========================\n",
    "text_chunks = []\n",
    "for rec in all_texts:\n",
    "    # semantic_chunker.split_text() returns list of string chunks\n",
    "    for i, c in enumerate(semantic_chunker.split_text(rec[\"text\"])):\n",
    "        cid = f\"{rec['paper_id']}_p{rec['page']}_c{i}\"\n",
    "        text_chunks.append({\n",
    "            \"id\": cid,\n",
    "            \"text\": c,\n",
    "            \"metadata\": {\n",
    "                \"paper_id\": rec[\"paper_id\"],\n",
    "                \"page\": rec[\"page\"],\n",
    "                \"chunk_index\": i,\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(\"Total semantic text chunks:\", len(text_chunks))\n",
    "\n",
    "# =========================\n",
    "# 8. Prepare image items\n",
    "# =========================\n",
    "image_items = []\n",
    "for itm in all_images:\n",
    "    iid = f\"{itm['paper_id']}_p{itm['page']}_img{itm['image_index']}\"\n",
    "    image_items.append({\n",
    "        \"id\": iid,\n",
    "        \"pil_image\": itm[\"pil_image\"],\n",
    "        \"metadata\": {\n",
    "            \"paper_id\": itm[\"paper_id\"],\n",
    "            \"page\": itm[\"page\"],\n",
    "            \"image_index\": itm[\"image_index\"],\n",
    "            \"type\": \"image\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(\"Total images:\", len(image_items))\n",
    "\n",
    "# =========================\n",
    "# 9. Initialize Pinecone indexes\n",
    "# =========================\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "def ensure_index(name: str, dim: int):\n",
    "    existing = [i[\"name\"] for i in pc.list_indexes().get(\"indexes\", [])]\n",
    "    if name not in existing:\n",
    "        print(\"Creating index:\", name)\n",
    "        pc.create_index(\n",
    "            name=name,\n",
    "            dimension=dim,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=PINECONE_REGION)\n",
    "        )\n",
    "        while not pc.describe_index(name).status[\"ready\"]:\n",
    "            time.sleep(1)\n",
    "    else:\n",
    "        print(\"Index exists:\", name)\n",
    "\n",
    "dim_text = len(embedding_model.embed_query(\"test\")) # Use embed_query for dimension\n",
    "dim_img = len(clip_model.encode([Image.new(\"RGB\", (10,10))])[0])\n",
    "\n",
    "ensure_index(INDEX_TEXT, dim_text)\n",
    "ensure_index(INDEX_IMAGE, dim_img)\n",
    "\n",
    "index_text = pc.Index(INDEX_TEXT)\n",
    "index_image = pc.Index(INDEX_IMAGE)\n",
    "\n",
    "print(\"Index stats (text):\", index_text.describe_index_stats())\n",
    "print(\"Index stats (image):\", index_image.describe_index_stats())\n",
    "\n",
    "# =========================\n",
    "# 10. Upsert embeddings\n",
    "# =========================\n",
    "BATCH = 64\n",
    "\n",
    "# Text embeddings\n",
    "for i in tqdm(range(0, len(text_chunks), BATCH), desc=\"Upserting text\"):\n",
    "    batch = text_chunks[i:i+BATCH]\n",
    "    texts_to_enc = [c[\"text\"] for c in batch]\n",
    "    embs = embedding_model.embed_documents(texts_to_enc) # Use embed_documents\n",
    "    vectors = []\n",
    "    for j, c in enumerate(batch):\n",
    "        vectors.append({\n",
    "            \"id\": c[\"id\"],\n",
    "            \"values\": embs[j],\n",
    "            \"metadata\": c[\"metadata\"] | {\"text\": c[\"text\"]}\n",
    "        })\n",
    "    index_text.upsert(vectors=vectors)\n",
    "\n",
    "# Image embeddings\n",
    "for i in tqdm(range(0, len(image_items), BATCH), desc=\"Upserting images\"):\n",
    "    batch = image_items[i:i+BATCH]\n",
    "    imgs = [itm[\"pil_image\"] for itm in batch]\n",
    "    embs = clip_model.encode(imgs, show_progress_bar=False)\n",
    "    vectors = []\n",
    "    for j, itm in enumerate(batch):\n",
    "        vectors.append({\n",
    "            \"id\": itm[\"id\"],\n",
    "            \"values\": embs[j].tolist(),\n",
    "            \"metadata\": itm[\"metadata\"]\n",
    "        })\n",
    "    index_image.upsert(vectors=vectors)\n",
    "\n",
    "print(\"Finished upserts\")\n",
    "\n",
    "# =========================\n",
    "# 11. Retrieval + answer generation\n",
    "# =========================\n",
    "def retrieve_multimodal(query: str, topk_t: int = 5, topk_i: int = 5):\n",
    "    results = []\n",
    "    # text side\n",
    "    q_emb_t = embedding_model.embed_query(query) # Use embed_query\n",
    "    res_t = index_text.query(vector=q_emb_t, top_k=topk_t, include_metadata=True)\n",
    "    for m in getattr(res_t, \"matches\", res_t.get(\"matches\", [])):\n",
    "        meta = m.metadata if hasattr(m, \"metadata\") else m.get(\"metadata\")\n",
    "        sc = m.score if hasattr(m, \"score\") else m.get(\"score\")\n",
    "        mid = m.id if hasattr(m, \"id\") else m.get(\"id\")\n",
    "        results.append({\"id\": mid, \"score\": sc, \"metadata\": meta, \"source\": \"text\"})\n",
    "\n",
    "    # image (cross-modal) side\n",
    "    q_emb_i = clip_model.encode([query], show_progress_bar=False)[0].tolist()\n",
    "    res_i = index_image.query(vector=q_emb_i, top_k=topk_i, include_metadata=True)\n",
    "    for m in getattr(res_i, \"matches\", res_i.get(\"matches\", [])):\n",
    "        meta = m.metadata if hasattr(m, \"metadata\") else m.get(\"metadata\")\n",
    "        sc = m.score if hasattr(m, \"score\") else m.get(\"score\")\n",
    "        mid = m.id if hasattr(m, \"id\") else m.get(\"id\")\n",
    "        results.append({\"id\": mid, \"score\": sc, \"metadata\": meta, \"source\": \"image\"})\n",
    "\n",
    "    # sort by score descending\n",
    "    results = [r for r in results if r[\"score\"] is not None]\n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return results\n",
    "\n",
    "# List available models\n",
    "for m in genai.list_models():\n",
    "  print(f\"Name: {m.name}\")\n",
    "  print(f\"Supported methods: {m.supported_generation_methods}\")\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "gem = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
    "\n",
    "def answer_from_query(query: str, topk_text: int = 5, topk_img: int = 3):\n",
    "    matches = retrieve_multimodal(query, topk_text, topk_img)\n",
    "    # build context from text matches\n",
    "    text_ctx = []\n",
    "    for m in matches:\n",
    "        if m[\"source\"] == \"text\":\n",
    "            txt = m[\"metadata\"].get(\"text\", \"\")\n",
    "            pid = m[\"metadata\"].get(\"paper_id\", \"\")\n",
    "            pg = m[\"metadata\"].get(\"page\", \"\")\n",
    "            ci = m[\"metadata\"].get(\"chunk_index\", \"\")\n",
    "            text_ctx.append(f\"Source: {pid}, page {pg}, chunk {ci}\\n{txt}\")\n",
    "    context = \"\\n\\n\".join(text_ctx) if text_ctx else \"No context found.\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful research assistant. Use only the context below to answer the question. If no answer is found, respond “I don’t know.”\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "    resp = gem.generate_content(prompt)\n",
    "    if hasattr(resp, \"text\"):\n",
    "        return resp.text, matches\n",
    "    else:\n",
    "        return resp.candidates[0].content.parts[0].text, matches\n",
    "\n",
    "# =========================\n",
    "# 12. Test\n",
    "# =========================\n",
    "q = \"Stem cell regeneration in microgravity\"\n",
    "ans, mm = answer_from_query(q, topk_text=5, topk_img=3)\n",
    "print(\"Answer:\\n\", ans)\n",
    "print(\"\\nTop matches:\")\n",
    "for m in mm[:8]:\n",
    "    print(m[\"source\"], m[\"id\"], m[\"score\"], m[\"metadata\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
